"""
LLM Benchmark API í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸
"""
import requests
import json
from typing import Dict, Any

class BenchmarkClient:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
    
    def run_benchmark(
        self,
        api_endpoint: str,
        api_key: str,
        model_name: str,
        test_prompts: list = None,
        num_requests: int = 10
    ) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        payload = {
            "api_endpoint": api_endpoint,
            "api_key": api_key,
            "model_name": model_name,
            "num_requests": num_requests
        }
        
        if test_prompts:
            payload["test_prompts"] = test_prompts
        
        response = requests.post(
            f"{self.base_url}/benchmark",
            json=payload,
            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        response.raise_for_status()
        return response.json()
    
    def print_results(self, result: Dict[str, Any]):
        """ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print("\n" + "="*60)
        print(f"ëª¨ë¸: {result['model_name']}")
        print(f"ì¸¡ì • ì‹œê°„: {result['timestamp']}")
        print("="*60)
        
        metrics = result['metrics']
        
        print("\nğŸ“Š ì „ì²´ í†µê³„")
        print(f"  ì´ ìš”ì²­ ìˆ˜: {metrics['total_requests']}")
        print(f"  ì„±ê³µ: {metrics['successful_requests']}")
        print(f"  ì‹¤íŒ¨: {metrics['failed_requests']}")
        print(f"  ì„±ê³µë¥ : {metrics['success_rate']:.2f}%")
        
        print("\nâ±ï¸  ì§€ì—° ì‹œê°„ (Latency)")
        print(f"  í‰ê· : {metrics['average_latency']:.3f}ì´ˆ")
        print(f"  ì¤‘ì•™ê°’: {metrics['median_latency']:.3f}ì´ˆ")
        print(f"  ìµœì†Œ: {metrics['min_latency']:.3f}ì´ˆ")
        print(f"  ìµœëŒ€: {metrics['max_latency']:.3f}ì´ˆ")
        print(f"  P95: {metrics['p95_latency']:.3f}ì´ˆ")
        print(f"  P99: {metrics['p99_latency']:.3f}ì´ˆ")
        
        print("\nğŸš€ ì²˜ë¦¬ëŸ‰ (Throughput)")
        print(f"  í‰ê·  í† í°/ì´ˆ: {metrics['average_tokens_per_second']:.2f}")
        print(f"  ì¤‘ì•™ê°’ í† í°/ì´ˆ: {metrics['median_tokens_per_second']:.2f}")
        print(f"  ìµœëŒ€ í† í°/ì´ˆ: {metrics['max_tokens_per_second']:.2f}")
        print(f"  ìš”ì²­/ì´ˆ: {metrics['requests_per_second']:.3f}")
        
        print("\nğŸ¯ í† í° ì‚¬ìš©ëŸ‰")
        print(f"  ì´ í† í°: {metrics['total_tokens_used']:,}")
        print(f"  í‰ê·  í† í°/ìš”ì²­: {metrics['average_tokens_per_request']:.1f}")
        print(f"  ì´ ì†Œìš” ì‹œê°„: {metrics['total_duration']:.2f}ì´ˆ")
        
        print("\n" + "="*60 + "\n")

def example_openai():
    """OpenAI API ë²¤ì¹˜ë§ˆí¬ ì˜ˆì œ"""
    client = BenchmarkClient()
    
    print("OpenAI API ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
    
    result = client.run_benchmark(
        api_endpoint="https://api.openai.com/v1/chat/completions",
        api_key="your-openai-api-key-here",  # ì‹¤ì œ API í‚¤ë¡œ ë³€ê²½
        model_name="gpt-3.5-turbo",
        num_requests=5
    )
    
    client.print_results(result)

def example_custom_prompts():
    """ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ë²¤ì¹˜ë§ˆí¬ ì˜ˆì œ"""
    client = BenchmarkClient()
    
    custom_prompts = [
        "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ì˜ ì¥ë‹¨ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "Dockerì™€ ê°€ìƒ ë¨¸ì‹ ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    ]
    
    print("ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ë¡œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
    
    result = client.run_benchmark(
        api_endpoint="https://api.openai.com/v1/chat/completions",
        api_key="your-api-key-here",  # ì‹¤ì œ API í‚¤ë¡œ ë³€ê²½
        model_name="gpt-3.5-turbo",
        test_prompts=custom_prompts,
        num_requests=9  # ê° í”„ë¡¬í”„íŠ¸ 3ë²ˆì”©
    )
    
    client.print_results(result)

def compare_models():
    """ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ ì˜ˆì œ"""
    client = BenchmarkClient()
    
    models = [
        ("gpt-3.5-turbo", "https://api.openai.com/v1/chat/completions"),
        ("gpt-4", "https://api.openai.com/v1/chat/completions"),
    ]
    
    results = []
    
    for model_name, endpoint in models:
        print(f"\n{model_name} ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘...")
        try:
            result = client.run_benchmark(
                api_endpoint=endpoint,
                api_key="your-api-key-here",  # ì‹¤ì œ API í‚¤ë¡œ ë³€ê²½
                model_name=model_name,
                num_requests=5
            )
            results.append((model_name, result))
            client.print_results(result)
        except Exception as e:
            print(f"âŒ {model_name} ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
    
    # ë¹„êµ ê²°ê³¼ ì¶œë ¥
    if len(results) > 1:
        print("\n" + "="*60)
        print("ëª¨ë¸ ë¹„êµ")
        print("="*60)
        print(f"{'ëª¨ë¸':<20} {'í‰ê·  ì§€ì—°ì‹œê°„':<15} {'í† í°/ì´ˆ':<15} {'ì„±ê³µë¥ ':<10}")
        print("-"*60)
        
        for model_name, result in results:
            metrics = result['metrics']
            print(f"{model_name:<20} "
                  f"{metrics['average_latency']:.3f}ì´ˆ{'':<8} "
                  f"{metrics['average_tokens_per_second']:.2f}{'':<8} "
                  f"{metrics['success_rate']:.1f}%")

if __name__ == "__main__":
    print("LLM Benchmark í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸\n")
    
    # ì˜ˆì œ ì‹¤í–‰ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
    # example_openai()
    # example_custom_prompts()
    # compare_models()
    
    print("ì‚¬ìš© ë°©ë²•:")
    print("1. llm_benchmark_api.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”")
    print("2. ì´ íŒŒì¼ì˜ ì˜ˆì œ í•¨ìˆ˜ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”")
    print("3. ì›í•˜ëŠ” ì˜ˆì œ í•¨ìˆ˜ì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”")
